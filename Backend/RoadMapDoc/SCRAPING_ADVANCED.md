# üï∑Ô∏è Scraping Avanc√© - Documentation

## Vue d'ensemble

PriceWatch impl√©mente plusieurs fonctionnalit√©s avanc√©es de scraping pour am√©liorer la fiabilit√©, les performances et √©viter les blocages :

- **Rotation des User-Agents** - √âvite la d√©tection comme bot
- **Cache Redis** - R√©duit les requ√™tes redondantes
- **Circuit Breaker** - Prot√®ge contre la surcharge des sites
- **Proxies rotatifs** - √âvite les blocages IP

---

## 1. Rotation des User-Agents

### Description

La rotation des User-Agents permet d'√©viter la d√©tection comme bot en variant les headers HTTP √† chaque requ√™te. PriceWatch utilise un pool de 15 User-Agents r√©alistes simulant diff√©rents navigateurs et syst√®mes d'exploitation.

### Utilisation

```python
from app.services.scraper_advanced import UserAgentRotator

# Obtenir un User-Agent al√©atoire
ua = UserAgentRotator.get_random()

# Obtenir des headers complets avec User-Agent
headers = UserAgentRotator.get_headers(include_full_headers=True)

# Obtenir uniquement le User-Agent
headers = UserAgentRotator.get_headers(include_full_headers=False)
```

### Pool de User-Agents

Le pool comprend :
- **Chrome** (Windows, macOS, Linux) - Versions 120, 121, 122
- **Firefox** (Windows, macOS, Linux) - Versions 121, 122
- **Safari** (macOS) - Version 17.1, 17.2
- **Edge** (Windows) - Versions 120, 121

### Int√©gration

La rotation est **automatique** dans `PriceScraper`. √Ä chaque tentative de scraping, un nouveau User-Agent est s√©lectionn√© al√©atoirement.

```python
# Automatiquement utilis√© dans scrape_product()
scraper = PriceScraper()
result = scraper.scrape_product(url)  # User-Agent rotatif appliqu√©
```

---

## 2. Cache Redis

### Description

Le cache Redis stocke les r√©sultats de scraping pour √©viter de refaire des requ√™tes inutiles vers les sites e-commerce. Chaque r√©sultat est mis en cache avec un TTL (Time-To-Live) configurable.

### Configuration

Variables d'environnement dans `.env` :

```env
SCRAPER_CACHE_ENABLED=true          # Activer/d√©sactiver le cache
SCRAPER_CACHE_TTL=3600              # Dur√©e de vie en secondes (1h par d√©faut)
```

### Utilisation

```python
from app.services.scraper_advanced import ScraperCache

# Initialisation
cache = ScraperCache(default_ttl=3600)

# V√©rifier le cache
cached_data = cache.get(url)
if cached_data:
    print(f"Cache HIT: {cached_data}")

# Mettre en cache un r√©sultat
data = {"name": "Product", "price": 99.99}
cache.set(url, data, ttl=1800)  # 30 minutes

# Invalider le cache pour une URL
cache.invalidate(url)

# Vider tout le cache scraper
cache.clear_all()
```

### Int√©gration

Le cache est **automatiquement utilis√©** dans `PriceScraper` :

```python
scraper = PriceScraper(use_cache=True, cache_ttl=3600)

# Premier appel : scraping r√©el
result1 = scraper.scrape_product(url)

# Deuxi√®me appel : r√©sultat depuis le cache (si < 1h)
result2 = scraper.scrape_product(url)

# Forcer un scraping frais (bypass cache)
result3 = scraper.scrape_product(url, bypass_cache=True)
```

### Avantages

- ‚úÖ **R√©duit la charge** sur les sites e-commerce
- ‚úÖ **Am√©liore les performances** (pas de requ√™te HTTP)
- ‚úÖ **√âvite les blocages** (moins de requ√™tes = moins suspicieux)
- ‚úÖ **√âconomise des ressources** (bande passante, CPU)

### Cl√©s de cache

Les cl√©s sont g√©n√©r√©es avec un hash MD5 de l'URL :
```
scraper_cache:<md5_hash>
```

Exemple :
```
scraper_cache:5d41402abc4b2a76b9719d911017c592
```

---

## 3. Circuit Breaker

### Description

Le Circuit Breaker impl√©mente le pattern de r√©silience pour prot√©ger les sites e-commerce contre la surcharge. Lorsqu'un site rencontre trop d'√©checs cons√©cutifs, le circuit s'ouvre automatiquement et bloque temporairement les requ√™tes.

### √âtats du Circuit

1. **CLOSED** (Ferm√©) - √âtat normal, requ√™tes autoris√©es
2. **OPEN** (Ouvert) - Trop d'√©checs, requ√™tes bloqu√©es
3. **HALF_OPEN** (Semi-ouvert) - Test de r√©cup√©ration

```
CLOSED ‚îÄ‚îÄ(5 √©checs)‚îÄ‚îÄ> OPEN ‚îÄ‚îÄ(60s timeout)‚îÄ‚îÄ> HALF_OPEN ‚îÄ‚îÄ(2 succ√®s)‚îÄ‚îÄ> CLOSED
   ‚îÇ                      ‚îÇ                         ‚îÇ
   ‚îÇ                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ(timeout pas √©coul√©)‚îÄ‚îò
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ(succ√®s)‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Configuration

Variables d'environnement :

```env
SCRAPER_CIRCUIT_BREAKER_ENABLED=true     # Activer/d√©sactiver
SCRAPER_CIRCUIT_BREAKER_THRESHOLD=5      # Nombre d'√©checs avant ouverture
SCRAPER_CIRCUIT_BREAKER_TIMEOUT=60       # Secondes avant tentative de r√©cup√©ration
```

### Utilisation

```python
from app.services.scraper_advanced import CircuitBreaker

# Initialisation
breaker = CircuitBreaker(
    failure_threshold=5,      # Ouvre apr√®s 5 √©checs
    recovery_timeout=60,      # 60s avant test de r√©cup√©ration
    success_threshold=2       # 2 succ√®s pour fermer
)

# V√©rifier si le site est disponible
if breaker.is_available("amazon"):
    try:
        # Effectuer le scraping
        result = scrape_amazon()
        breaker.record_success("amazon")  # Enregistrer le succ√®s
    except Exception:
        breaker.record_failure("amazon")  # Enregistrer l'√©chec
else:
    print("Circuit OPEN - requ√™tes bloqu√©es pour amazon")

# R√©initialiser manuellement un circuit
breaker.reset("amazon")
```

### Int√©gration

Le circuit breaker est **automatiquement g√©r√©** dans `PriceScraper` :

```python
scraper = PriceScraper(use_circuit_breaker=True)

# Le circuit breaker v√©rifie automatiquement l'√©tat avant chaque scraping
result = scraper.scrape_product(url)
# Si circuit OPEN -> retourne None
# Si circuit CLOSED/HALF_OPEN -> tente le scraping
```

### Suivi par site

Le circuit breaker suit l'√©tat **par site e-commerce** :
- `amazon` (Amazon.fr, .com, .de, etc.)
- `fnac`
- `darty`
- `cdiscount`
- `boulanger`
- `leclerc`

Chaque site a son propre circuit ind√©pendant.

### Avantages

- ‚úÖ **√âvite la surcharge** des sites
- ‚úÖ **R√©duit les erreurs en cascade** (fail fast)
- ‚úÖ **Auto-r√©cup√©ration** apr√®s timeout
- ‚úÖ **Prot√®ge l'application** contre les bans IP

---

## 4. Proxies Rotatifs

### Description

Les proxies rotatifs permettent de faire passer les requ√™tes de scraping par diff√©rentes adresses IP, √©vitant ainsi les blocages bas√©s sur l'IP source.

### Configuration

Variables d'environnement :

```env
SCRAPER_PROXY_ENABLED=false                        # Activer/d√©sactiver
PROXY_LIST=http://proxy1:8080,http://proxy2:8080  # Liste de proxies
```

Format des proxies :
```
http://ip:port
http://username:password@ip:port
https://ip:port
```

### Utilisation

```python
from app.services.scraper_advanced import ProxyRotator

# Initialisation avec liste de proxies
proxies = [
    "http://proxy1.example.com:8080",
    "http://user:pass@proxy2.example.com:8080"
]
rotator = ProxyRotator(proxy_list=proxies)

# Obtenir le prochain proxy (rotation)
proxy = rotator.get_next()

# Obtenir un proxy al√©atoire
proxy = rotator.get_random()

# Obtenir un dict pour requests
proxies_dict = rotator.get_proxies_dict()
# Retourne: {"http": "...", "https": "..."}

# Ajouter/supprimer des proxies dynamiquement
rotator.add_proxy("http://newproxy:8080")
rotator.remove_proxy("http://oldproxy:8080")
```

### Int√©gration

Les proxies sont **automatiquement utilis√©s** dans `PriceScraper` :

```python
scraper = PriceScraper(use_proxy=True)

# Les proxies sont automatiquement rot√©s √† chaque requ√™te
result = scraper.scrape_product(url)
```

### S√©lection des proxies

Deux modes disponibles :
1. **Rotation s√©quentielle** - `get_next()` : 1 ‚Üí 2 ‚Üí 3 ‚Üí 1 ‚Üí ...
2. **S√©lection al√©atoire** - `get_random()` : choix al√©atoire √† chaque appel

`PriceScraper` utilise la **s√©lection al√©atoire** par d√©faut.

### ‚ö†Ô∏è Important

- Les proxies doivent √™tre **fiables et rapides**
- Les proxies gratuits peuvent √™tre **lents ou instables**
- Privil√©giez des proxies **r√©sidentiels** pour le scraping e-commerce
- Testez vos proxies avant de les ajouter

---

## üîß Configuration Globale

### Fichier `.env`

```env
# ===== SCRAPING AVANC√â =====

# Cache Redis
SCRAPER_CACHE_ENABLED=true
SCRAPER_CACHE_TTL=3600

# Circuit Breaker
SCRAPER_CIRCUIT_BREAKER_ENABLED=true
SCRAPER_CIRCUIT_BREAKER_THRESHOLD=5
SCRAPER_CIRCUIT_BREAKER_TIMEOUT=60

# Proxies
SCRAPER_PROXY_ENABLED=false
PROXY_LIST=
```

### Exemple d'utilisation compl√®te

```python
from app.services.scraper import PriceScraper

# Initialisation avec toutes les fonctionnalit√©s avanc√©es
scraper = PriceScraper(
    max_retries=3,
    retry_delay=2,
    use_cache=True,             # Utilise le cache Redis
    cache_ttl=3600,             # 1 heure de cache
    use_circuit_breaker=True,   # Active le circuit breaker
    use_proxy=False             # Proxies d√©sactiv√©s (pas n√©cessaire par d√©faut)
)

# Scraping avec toutes les fonctionnalit√©s
url = "https://www.amazon.fr/dp/B08N5WRWNW"
try:
    result = scraper.scrape_product(url)
    if result:
        print(f"‚úÖ Scraped: {result.name} - ‚Ç¨{result.price}")
    else:
        print("‚ùå Scraping failed (circuit breaker open?)")
except Exception as e:
    print(f"‚ùå Error: {e}")
```

---

## üìä Monitoring

### Logs

Les fonctionnalit√©s avanc√©es g√©n√®rent des logs d√©taill√©s :

```
[INFO] PriceScraper initialized (cache=True, circuit_breaker=True, proxy=False)
[INFO] Cache HIT for URL: https://www.amazon.fr/...
[DEBUG] Selected User-Agent: Mozilla/5.0 (Windows NT...)
[INFO] Circuit CLOSED for 'amazon' - requests allowed
[WARNING] Circuit failure for 'amazon': 3/5
[WARNING] Circuit OPEN for 'amazon' - requests blocked
[INFO] Circuit moving to HALF_OPEN state (recovery attempt)
```

### M√©triques Redis

Vous pouvez surveiller les √©tats du circuit breaker dans Redis :

```bash
# Liste des cl√©s circuit breaker
redis-cli KEYS "circuit_breaker:*"

# √âtat d'un circuit
redis-cli GET "circuit_breaker:amazon:state"

# Nombre d'√©checs
redis-cli GET "circuit_breaker:amazon:failures"
```

---

## üß™ Tests

Tests unitaires disponibles dans `tests/test_unit_scraper_advanced.py` :

```bash
# Ex√©cuter tous les tests
pytest tests/test_unit_scraper_advanced.py -v

# Tests sp√©cifiques
pytest tests/test_unit_scraper_advanced.py::TestUserAgentRotator -v
pytest tests/test_unit_scraper_advanced.py::TestScraperCache -v
pytest tests/test_unit_scraper_advanced.py::TestCircuitBreaker -v
pytest tests/test_unit_scraper_advanced.py::TestProxyRotator -v
```

**Couverture** : 41 tests, 100% de couverture

---

## üöÄ Bonnes Pratiques

### 1. Cache

- ‚úÖ Utilisez le cache pour les v√©rifications fr√©quentes
- ‚úÖ Ajustez le TTL selon vos besoins (1h par d√©faut)
- ‚úÖ Utilisez `bypass_cache=True` pour les v√©rifications manuelles
- ‚ùå N'utilisez pas de TTL trop long (donn√©es obsol√®tes)

### 2. Circuit Breaker

- ‚úÖ Gardez les valeurs par d√©faut sauf besoins sp√©cifiques
- ‚úÖ Surveillez les logs pour d√©tecter les circuits ouverts
- ‚úÖ R√©initialisez manuellement si n√©cessaire
- ‚ùå Ne d√©sactivez pas sauf pour debug

### 3. User-Agent

- ‚úÖ Laissez la rotation activ√©e (automatique)
- ‚úÖ Le pool est d√©j√† optimis√©
- ‚ùå N'ajoutez pas de User-Agents suspects ou obsol√®tes

### 4. Proxies

- ‚úÖ Utilisez uniquement si n√©cessaire (blocages IP fr√©quents)
- ‚úÖ Testez vos proxies avant ajout
- ‚úÖ Privil√©giez des proxies r√©sidentiels
- ‚ùå N'utilisez pas de proxies gratuits en production

---

## üìö R√©f√©rences

- **Pattern Circuit Breaker** : [Martin Fowler](https://martinfowler.com/bliki/CircuitBreaker.html)
- **Redis Caching** : [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- **User-Agent Strings** : [WhatIsMyBrowser](https://www.whatismybrowser.com/guides/the-latest-user-agent/)

---

**Derni√®re mise √† jour** : 2025-12-23
